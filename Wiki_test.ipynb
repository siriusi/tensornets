{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWUdwHrBkctr"
   },
   "source": [
    "# CIFAR10_Test\n",
    "\n",
    "使用CIFAR10数据集对CNN进行训练及测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Ykfy4xGQkct0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "import tensornets as nets\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 125,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 31366,
     "status": "ok",
     "timestamp": 1522747101694,
     "user": {
      "displayName": "Y Zzhuangy",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "115487754759868122157"
     },
     "user_tz": -480
    },
    "id": "zG95i-smkcuA",
    "outputId": "e588e385-dee3-40c0-cb6f-e792f146bc08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (10000, 32, 32, 3)\n",
      "Train labels shape:  (10000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "import sys\n",
    "if sys.platform == \"linux\" :\n",
    "    cifar10_dir = \"/home/z_tomcato/cs231n/assignment2/assignment2/cs231n/datasets/cifar-10-batches-py\"\n",
    "else:\n",
    "    cifar10_dir = 'cs231n/datasets'\n",
    "    \n",
    "def get_CIFAR10_data(num_training=10000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    # cifar10_dir = '../assignment2/cs231n/datasets'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensornets测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\\n\\n#model = nets.InceptionResNet2\\nmodel = nets.MobileNet100\\n\\nnewModel = model(inputs)\\nimg = nets.utils.load_img('cat.png', target_size=256, crop_size=224)\\nassert img.shape == (1, 224, 224, 3)\\nwith tf.Session() as sess:\\n    img = newModel.preprocess(img)  # equivalent to img = nets.preprocess(model, img)\\n    sess.run(newModel.pretrained())  # equivalent to nets.pretrained(model)\\n    scores = sess.run(newModel, {inputs: img})\\n    #rint(preds)\\nprint(nets.utils.decode_predictions(scores, top=1)[0])\\n#print(tf.argmax(tf.squeeze(preds, [0], axis = 1)))\\npredictions = tf.argmax(scores, axis = 1)\\nprint(predictions)\\n[(u'n02124075', u'Egyptian_cat', 0.28067636), (u'n02127052', u'lynx', 0.16826575)]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "\n",
    "#model = nets.InceptionResNet2\n",
    "model = nets.MobileNet100\n",
    "\n",
    "newModel = model(inputs)\n",
    "img = nets.utils.load_img('cat.png', target_size=256, crop_size=224)\n",
    "assert img.shape == (1, 224, 224, 3)\n",
    "with tf.Session() as sess:\n",
    "    img = newModel.preprocess(img)  # equivalent to img = nets.preprocess(model, img)\n",
    "    sess.run(newModel.pretrained())  # equivalent to nets.pretrained(model)\n",
    "    scores = sess.run(newModel, {inputs: img})\n",
    "    #rint(preds)\n",
    "print(nets.utils.decode_predictions(scores, top=1)[0])\n",
    "#print(tf.argmax(tf.squeeze(preds, [0], axis = 1)))\n",
    "predictions = tf.argmax(scores, axis = 1)\n",
    "print(predictions)\n",
    "[(u'n02124075', u'Egyptian_cat', 0.28067636), (u'n02127052', u'lynx', 0.16826575)]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_img_from_tensor(x, target_size=None, crop_size=None, interp=cv2.INTER_CUBIC):\n",
    "\n",
    "    minSize = min(x.shape[1:3])\n",
    "    imgs = None\n",
    "    if target_size:\n",
    "        if isinstance(target_size, int):\n",
    "            hw_tuple = (x.shape[1] * target_size // minSize, x.shape[2] * target_size // minSize)\n",
    "        else:\n",
    "            hw_tuple = (target_size[1], target_size[0])\n",
    "        imgs = np.zeros((x.shape[0],hw_tuple[0],hw_tuple[1], 3), dtype=np.uint8)\n",
    "        if x.shape[1:3] != hw_tuple:\n",
    "            for i in range(x.shape[0]):\n",
    "                imgs[i,:, :, :] = cv2.resize(x[i, :, :, :], hw_tuple, interpolation=interp)\n",
    "    if crop_size is not None:\n",
    "        imgs = nets.utils.crop(imgs, crop_size)\n",
    "        \n",
    "    return imgs\n",
    "\n",
    "def img_preprocess(x):\n",
    "    # Copied from keras (equivalent to the same as in TF Slim)\n",
    "    x = x.copy()\n",
    "    x = x / 255.\n",
    "    x = x - 0.5\n",
    "    x = x * 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#model = nets.MobileNet100\n",
    "\n",
    "def run_model(session, Xd, yd, Xv, yv, num_class = 10, epochs=3, batch_size=100,print_every=10, learning_rate = 1e-5, dropout = 0.5):\n",
    "    print(\"Batch dataset initialized.\\n# of training data: {}\\n# of test data: {}\\n# of class: {}\"\n",
    "          .format(Xd.shape[0], Xv.shape[0], 10))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        inputs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        outputs = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        cnn_net = nets.MobileNet100(inputs, is_training = True, classes = num_class)\n",
    "        \n",
    "        cnn_loss = tf.losses.softmax_cross_entropy(tf.one_hot(outputs,num_class, dtype=tf.int32), cnn_net)\n",
    "        cnn_train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cnn_loss)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        nets.pretrained(cnn_net)        \n",
    "        \n",
    "        # tensorboard setting\n",
    "\n",
    "        fileName = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "        fileName = os.path.normcase(\"./result/\" + fileName)\n",
    "        summary_writer = tf.summary.FileWriter(fileName, sess.graph)\n",
    "        \n",
    "        global_step = 0\n",
    "                  \n",
    "        for current_epoch in range(epochs):\n",
    "            # training step\n",
    "            ###for x_batch, y_batch in batch_set.batches():\n",
    "            print(\"#############################Epoch Start##############################\")\n",
    "            \n",
    "            for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "                start = time.time()\n",
    "                start_idx = (i*batch_size)%Xd.shape[0]\n",
    "                idx = np.int32(train_indicies[start_idx:start_idx+batch_size])\n",
    "                                      \n",
    "                batch_Xd = load_img_from_tensor(Xd[idx,:, :, :], target_size=256, crop_size=224)\n",
    "                batch_Xd = cnn_net.preprocess(batch_Xd) \n",
    "                batch_yd = yd[idx]\n",
    "                feed = {inputs : batch_Xd, outputs : batch_yd}                \n",
    "                \n",
    "                global_step = global_step + 1\n",
    "                \n",
    "                cnn_predictions = tf.argmax(cnn_net, axis = 1)\n",
    "                cnn_correct_prediction = tf.equal(tf.cast(cnn_predictions, dtype=tf.int32), batch_yd)\n",
    "                cnn_accuracy = tf.reduce_mean(tf.cast(cnn_correct_prediction, tf.float32))\n",
    "        \n",
    "                train_summary = tf.summary.merge([tf.summary.scalar(\"train_loss\", cnn_loss),\n",
    "                                          tf.summary.scalar(\"train_accuracy\", cnn_accuracy)])\n",
    "\n",
    "                _, loss, scores,accuracy, summary = sess.run([cnn_train, cnn_loss, \n",
    "                                                              cnn_net, cnn_accuracy, train_summary], feed_dict=feed)\n",
    "                \n",
    "                summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "                \n",
    "                if global_step % print_every == 0:\n",
    "                    print(\"{}/{} ({} epochs) step, loss : {:.6f}, accuracy : {:.3f}, time/batch : {:.3f}sec\"\n",
    "                          .format(global_step, int(round(Xd.shape[0]/batch_size)) * epochs, current_epoch,\n",
    "                                  loss, accuracy, time.time() - start))\n",
    "\n",
    "            # test step\n",
    "            start, avg_loss, avg_accuracy = time.time(), 0, 0\n",
    "            \n",
    "            test_summary = tf.summary.merge([tf.summary.scalar(\"val_loss\", cnn_loss),\n",
    "                                             tf.summary.scalar(\"val_accuracy\", cnn_net)])\n",
    "            \n",
    "            Xv = cnn_net.preprocess(Xv) \n",
    "            feed = {inputs : Xv, outputs : yv}\n",
    "            loss, accuracy, summary = sess.run([cnn_loss, cnn_accuracy, test_summary], feed_dict=feed)\n",
    "\n",
    "            summary_writer.add_summary(summary, current_epoch)\n",
    "            print(\"{} epochs test result. loss : {:.6f}, accuracy : {:.3f}, time/batch : {:.3f}sec\"\n",
    "                  .format(current_epoch, loss , accuracy , time.time() - start))\n",
    "            \n",
    "            print(\"\\n\")\n",
    "    return \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_wiki(wiki_path, num_training=49000, num_validation=1000, num_test=1000,):\n",
    "   \n",
    "    wiki_path = \"\"\n",
    "    if sys.platform == \"linux\" :\n",
    "            wiki_path = \"/devdata/wiki/\"\n",
    "    else:\n",
    "            wiki_path = \"G:\\\\MachineLearning\\\\wiki\\\\wiki\\\\\"\n",
    "\n",
    "    mat_path = wiki_path + 'wiki_with_age.mat'\n",
    "    data = sio.loadmat(mat_path) \n",
    "    \n",
    "    img_paths = []\n",
    "    for i in range(len(wiki_data[6][0])):\n",
    "        wiki_data = data['wiki'][0][0]\n",
    "        face_score =wiki_data[6][0][i]\n",
    "        if face_score != float(\"-inf\"):\n",
    "            full_path = wiki_path + wiki_data[0][0][2][0][i][0]\n",
    "            img = cv2.imread(full_path)\n",
    "            cv2.imshow(\"test\", img)\n",
    "\n",
    "            face_loc = wiki_data[5][0][i][0]\n",
    "            print(face_loc)\n",
    "            face_loc = face_loc.astype(\"int32\")\n",
    "            print(face_loc)\n",
    "            roi_img = img[face_loc[1]:face_loc[3], face_loc[0]:face_loc[2]]\n",
    "            temp_img = cv2.resize(roi_img, ((face_loc[3]-face_loc[1]) * 3, (face_loc[2]-face_loc[0]) * 3))\n",
    "            cv2.imshow(\"temp_img\", temp_img)\n",
    "            cv2.imshow(\"roi_img\", roi_img)\n",
    "            print(\"age: \", wiki_data[8][0][i])\n",
    "            gender = wiki_data[3][0][i]\n",
    "            if gender == 0:\n",
    "                print(\"女\")\n",
    "            else:\n",
    "                print(\"男\")\n",
    "        else:\n",
    "            print(\"没找到人脸\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dataset initialized.\n",
      "# of training data: 10000\n",
      "# of test data: 1000\n",
      "# of class: 10\n",
      "#############################Epoch Start##############################"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #print('Training')\n",
    "    \n",
    "    run_model(sess,X_train,y_train,X_val,y_val, epochs=4, batch_size=500,print_every=100, learning_rate = 1e-5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "TensorFlow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
